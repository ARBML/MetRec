{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AraMeter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zaidalyafeai/AraMeter/blob/master/AraMeter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d0vjDuxuHl8",
        "colab_type": "code",
        "outputId": "b9cd3184-c91d-4732-fe7c-6458acee78e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!pip install pyarabic"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyarabic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/59/2c7efe30a789c1dfd3c5c15b9b06fcde8cde67ff1c27adabb78692eb7f7f/PyArabic-0.6.7.tar.gz (103kB)\n",
            "\r\u001b[K     |███▏                            | 10kB 18.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 40kB 5.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 81kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 92kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 102kB 6.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 6.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyarabic\n",
            "  Building wheel for pyarabic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyarabic: filename=PyArabic-0.6.7-cp36-none-any.whl size=108603 sha256=f3fb95e68288f22ca449426203bc450892af93b6fb1625538a3d5c25d784d14f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/50/b1/4df7f705f36e91360ab04416dbf1017084698d30a7a3645b5e\n",
            "Successfully built pyarabic\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klf1-MC3DEhi",
        "colab_type": "text"
      },
      "source": [
        "We use a product review dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvOJw5Bg6c5J",
        "colab_type": "code",
        "outputId": "da806b88-6331-41e7-816a-0e77cbaa4fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!wget 'https://raw.githubusercontent.com/zaidalyafeai/ARBML/master/datasets/Poem Meters/baits.zip'\n",
        "!unzip baits.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-19 11:49:56--  https://raw.githubusercontent.com/zaidalyafeai/ARBML/master/datasets/Poem%20Meters/baits.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2267882 (2.2M) [application/zip]\n",
            "Saving to: ‘baits.zip’\n",
            "\n",
            "\rbaits.zip             0%[                    ]       0  --.-KB/s               \rbaits.zip           100%[===================>]   2.16M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-05-19 11:49:57 (21.7 MB/s) - ‘baits.zip’ saved [2267882/2267882]\n",
            "\n",
            "Archive:  baits.zip\n",
            "   creating: final_baits/\n",
            "  inflating: final_baits/train.txt   \n",
            "  inflating: final_baits/labels.txt  \n",
            "  inflating: final_baits/test.txt    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwlvjSR-DS15",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23FSFg5t6fc1",
        "colab_type": "code",
        "outputId": "9ebe7f56-efc0-4766-c022-9d4cf6fe5114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import glob\n",
        "from random import shuffle\n",
        "from pyarabic import araby\n",
        "from tensorflow.keras.layers import GRU, Embedding, Dense, Input, Dropout, Bidirectional, BatchNormalization, Flatten, Reshape\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZEFvXv2SqUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('final_baits/labels.txt', 'r') as f:\n",
        "  label2name = f.readlines()\n",
        "  label2name = [name.replace('\\n', '') for name in label2name]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfbnvdT4Cmz0",
        "colab_type": "text"
      },
      "source": [
        "## Read the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKhqB_MfCjEP",
        "colab_type": "text"
      },
      "source": [
        "preprocess a review by removing special characters and long spaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7MjMLLn6gtK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "def extract_data(path, thresh = 70, on_shatrs = False):\n",
        "  global vocab\n",
        "  \n",
        "  text = \"\"\n",
        "  \n",
        "  X = []\n",
        "  y = []\n",
        "    \n",
        "  t = open(path, 'r').read()\n",
        "  t = araby.strip_tashkeel(t)\n",
        "  \n",
        "  # remove some exteranous chars \n",
        "  execluded = '!()*-ـ.:=o[]«»;؛,،~?؟\\u200f\\ufeffـ'\n",
        "  out = \"\"\n",
        "  \n",
        "  for char in t:\n",
        "    if char not in execluded:\n",
        "      out += char\n",
        "      \n",
        "  text += out\n",
        "  baits = out.split('\\n')\n",
        "  for line in baits:\n",
        "    if len(line) <= 1:\n",
        "      continue\n",
        "    label, bait = line.split(' ', 1)\n",
        "    label = int(label)\n",
        "\n",
        "    bait  = bait.strip()\n",
        "    if on_shatrs:\n",
        "      shatrs = bait.split('#')\n",
        "      for shatr in shatrs:\n",
        "        X.append(shatr.strip())\n",
        "        y.append(label)\n",
        "    else:\n",
        "      X.append(bait.strip())\n",
        "      y.append(label)\n",
        "  \n",
        "  #create the vocab \n",
        "  vocab = sorted(set(' '.join(X)))  \n",
        "  \n",
        "  #shuffle the data \n",
        "  X, y = shuffle(X, y)\n",
        "  return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWwTc-z2fv69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = extract_data(\"final_baits/train.txt\", on_shatrs=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7oXawNaVyEL",
        "colab_type": "code",
        "outputId": "20f58ae1-6a09-4ddb-db57-f7fe62c6ec98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for i in range(5):\n",
        "  print(X[i], ' ', label2name[y[i]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "وإلى كم شرائي بالجر منه # وانصرافي بخاطر مكسور   khafeef\n",
            "قل للمعنف عن هواه بجهله # زد من ملامك يا عذول وعنف   kamel\n",
            "سرى تطوى به الفلوات طيا # ونعم الذخر في يوم النشور   wafer\n",
            "لما غدت صعدته منآدة # ثقفتم المائل من صعاده   rajaz\n",
            "مولى ملوك الأنام قاطبة # فيمم البحر وامض عن قلبه   munsareh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syg_fvtnC1AK",
        "colab_type": "text"
      },
      "source": [
        "## Create Sequences\n",
        "Create sequences by using the most repeated 500 words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq0Ber9ICcWb",
        "colab_type": "text"
      },
      "source": [
        "## Create Numpy Arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROef8aerf8ar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_valid , y_train, y_valid = train_test_split(X, y, test_size = 0.15, random_state = 41)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63NiojywQ18F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i+1 for i, u in enumerate(vocab)}\n",
        "\n",
        "def to_sequences(X):\n",
        "  X = [[char2idx[char] for char in line] for line in X]\n",
        "  X = pad_sequences(X, padding='post', value=0, maxlen = 100)\n",
        "  return X\n",
        " \n",
        "X_train = to_sequences(X_train)\n",
        "X_valid = to_sequences(X_valid)\n",
        "\n",
        "y_train = np.array(y_train)\n",
        "y_valid = np.array(y_valid)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uHdRK4cCrGJ",
        "colab_type": "text"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3u3OxEcBfJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Input((100,)))\n",
        "model.add(Embedding(len(char2idx)+1, 256))\n",
        "model.add(Bidirectional(GRU(units = 256, return_sequences=True)))\n",
        "model.add(Bidirectional(GRU(units = 256, return_sequences=True)))\n",
        "model.add(Bidirectional(GRU(units = 256)))\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(len(label2name), activation = 'softmax'))\n",
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7imJBjJHxK1-",
        "colab_type": "code",
        "outputId": "42a9b432-3cec-44d5-9af7-3da1eb72105f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 256)          9984      \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 100, 512)          789504    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 100, 512)          1182720   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 512)               1182720   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 14)                1806      \n",
            "=================================================================\n",
            "Total params: 3,232,398\n",
            "Trainable params: 3,232,398\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPyUsGe_u9tw",
        "colab_type": "code",
        "outputId": "921fdf7c-c163-4981-b02b-2402be47d539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model(tf.zeros((10, 100))).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([10, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7U36aDCtQu",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEHRYgLhkozM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_delta=0.0001, min_lr=0.0001)]\n",
        "callbacks += [tf.keras.callbacks.ModelCheckpoint('full_verse.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52Ew-5ZyC3Nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_train, y_train, validation_data= (X_valid, y_valid), epochs = 15, batch_size= 128, shuffle = True, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVnnbxyUgDQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model('full_verse.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwZQrxhdDV4r",
        "colab_type": "text"
      },
      "source": [
        "## Tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q-Texz3DQsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classify(sentence):\n",
        "#   sentence = process_review(sentence)\n",
        "  sentence = araby.strip_tashkeel(sentence)\n",
        "  sequence = [char2idx[char] for char in sentence]\n",
        "  sequence = pad_sequences([sequence], maxlen = X_train.shape[1], padding='post', value=0)\n",
        "\n",
        "  pred = model.predict(sequence)[0]\n",
        "  print(label2name[np.argmax(pred, 0).astype('int')], np.max(pred))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMgcfGkZRLF2",
        "colab_type": "code",
        "outputId": "a0e20dc1-cf07-421e-b585-7948168dd485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "classify(\"ما تردون على هذا المحب # دائبا يشكو إليكم في الكتب\")\n",
        "classify(\"ولد الهدى فالكائنات ضياء # وفم الزمان تبسم وسناء\")\n",
        "classify(\" لك يا منازل في القلوب منازل # أقفرت أنت وهن منك أواهل\")\n",
        "classify(\"ومن لم يمت بالسيف مات بغيره # تعددت الأسباب والموت واحد\")\n",
        "classify(\"أنا النبي لا كذب # أنا ابن عبد المطلب\")\n",
        "classify(\"هذه دراهم اقفرت # أم ربور محتها الدهور\")\n",
        "classify(\"هزجنا في بواديكم # فأجزلتم عطايانا\")\n",
        "classify(\"بحر سريع ماله ساحل # مستفعلن مستفعلن فاعلن\")\n",
        "classify(\"مَا مَضَى فَاتَ وَالْمُؤَمَّلُ غَيْبٌ # وَلَكَ السَّاعَةُ الَّتِيْ أَنْتَ فِيْهَا\")\n",
        "classify(\"يا ليلُ الصبّ متى غدهُ # أقيامُ الساعة موعدهُ\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ramal 0.9999938\n",
            "kamel 0.99530077\n",
            "kamel 0.99872977\n",
            "taweel 0.9999771\n",
            "hazaj 0.5043862\n",
            "madeed 0.527348\n",
            "hazaj 0.9807406\n",
            "saree 0.9985551\n",
            "khafeef 0.9998852\n",
            "mutadarak 0.99972767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0C1vwNk1du8",
        "colab_type": "code",
        "outputId": "c91cb9a9-872f-481e-d1f3-3e850b7d0a98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!zip model.zip full_verse.h5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: full_verse.h5 (deflated 7%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ_Qh6OI1qP5",
        "colab_type": "code",
        "outputId": "b4ff37cf-9721-491a-e6fc-bbd18a59150e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 75404\n",
            "-rw-r--r-- 1 root root  2267882 May 19 11:49 baits.zip\n",
            "drwxr-xr-x 2 root root     4096 Aug  1  2019 final_baits\n",
            "-rw-r--r-- 1 root root 38894480 May 19 11:58 full_verse.h5\n",
            "-rw-r--r-- 1 root root 36038534 May 19 12:09 model.zip\n",
            "drwxr-xr-x 1 root root     4096 May 13 16:29 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRYJoGMd1q1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}